{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Lou1sM/meaningful_image_complexity"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cyR8gyGboL2O",
        "outputId": "54aa853e-5c93-4a3e-fbc5-cd58a76a62f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'meaningful_image_complexity'...\n",
            "remote: Enumerating objects: 314, done.\u001b[K\n",
            "remote: Counting objects: 100% (314/314), done.\u001b[K\n",
            "remote: Compressing objects: 100% (194/194), done.\u001b[K\n",
            "remote: Total 314 (delta 190), reused 230 (delta 116), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (314/314), 86.25 KiB | 5.07 MiB/s, done.\n",
            "Resolving deltas: 100% (190/190), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/pytorch/vision.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSUn8Bm0IGoy",
        "outputId": "eb5d4352-9f66-40c2-b427-e5e305e17571"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'vision'...\n",
            "remote: Enumerating objects: 584408, done.\u001b[K\n",
            "remote: Counting objects: 100% (32194/32194), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1892/1892), done.\u001b[K\n",
            "remote: Total 584408 (delta 30158), reused 32042 (delta 30073), pack-reused 552214 (from 1)\u001b[K\n",
            "Receiving objects: 100% (584408/584408), 1.09 GiB | 25.71 MiB/s, done.\n",
            "Resolving deltas: 100% (546249/546249), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas scipy matplotlib scikit-fuzzy scikit-image dl-utils385 --q"
      ],
      "metadata": {
        "id": "cc8EFqdJovdn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f753cadb-e232-4d9f-cd3e-46417c049ec0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/920.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m920.8/920.8 kB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install roboflow\n",
        "\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"cniNWVazrZnzZa0rBAup\")\n",
        "project = rf.workspace(\"military-technical-academy-86a4o\").project(\"uxo-app-detection\")\n",
        "version = project.version(55)\n",
        "dataset = version.download(\"yolov8\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-sF3UryoYW8",
        "outputId": "d99359fe-89a2-45fc-b09a-05f7509eddb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting roboflow\n",
            "  Downloading roboflow-1.1.48-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from roboflow) (2024.8.30)\n",
            "Collecting idna==3.7 (from roboflow)\n",
            "  Downloading idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: cycler in /usr/local/lib/python3.10/dist-packages (from roboflow) (0.12.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.4.7)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from roboflow) (3.8.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.26.4)\n",
            "Requirement already satisfied: opencv-python-headless==4.10.0.84 in /usr/local/lib/python3.10/dist-packages (from roboflow) (4.10.0.84)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from roboflow) (10.4.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.8.2)\n",
            "Collecting python-dotenv (from roboflow)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.32.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.16.0)\n",
            "Requirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.2.3)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from roboflow) (4.66.6)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from roboflow) (6.0.2)\n",
            "Requirement already satisfied: requests-toolbelt in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.0.0)\n",
            "Collecting filetype (from roboflow)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (1.3.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (4.54.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (3.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->roboflow) (3.4.0)\n",
            "Downloading roboflow-1.1.48-py3-none-any.whl (80 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.3/80.3 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading idna-3.7-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: filetype, python-dotenv, idna, roboflow\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.10\n",
            "    Uninstalling idna-3.10:\n",
            "      Successfully uninstalled idna-3.10\n",
            "Successfully installed filetype-1.2.0 idna-3.7 python-dotenv-1.0.1 roboflow-1.1.48\n",
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading Dataset Version Zip in UXO-App-Detection-55 to yolov8:: 100%|██████████| 3436076/3436076 [01:18<00:00, 43628.71it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Extracting Dataset Version Zip to UXO-App-Detection-55 in yolov8:: 100%|██████████| 6285/6285 [00:17<00:00, 350.76it/s] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Custom Loader"
      ],
      "metadata": {
        "id": "OzceZG1socD3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "from torchvision.transforms import functional as F\n",
        "from torchvision import transforms\n",
        "from vision.references.detection.transforms import SimpleCopyPaste\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    return tuple(zip(*batch))\n",
        "\n",
        "# Detection Input Size\n",
        "input_size = (256, 256)\n",
        "\n",
        "# CTX-UXO Dataset Polygon (Segmentents) to BBoxes\n",
        "def polygon_to_bbox(polygon):\n",
        "    x_coords = polygon[0::2]\n",
        "    y_coords = polygon[1::2]\n",
        "    x_min, x_max = min(x_coords), max(x_coords)\n",
        "    y_min, y_max = min(y_coords), max(y_coords)\n",
        "    return [x_min, y_min, x_max, y_max]\n",
        "\n",
        "# Convert YOLO to TorchVision\n",
        "def load_yolo_labels_with_polygons(label_path, image_shape):\n",
        "    labels = []\n",
        "    boxes = []\n",
        "    with open(label_path, 'r') as f:\n",
        "        for line in f.readlines():\n",
        "            data = line.strip().split()\n",
        "            class_id = int(data[0])\n",
        "            polygon = list(map(float, data[1:]))\n",
        "            bbox = polygon_to_bbox(polygon)\n",
        "\n",
        "\n",
        "            # Rescalează bounding box-ul la dimensiunea specificată\n",
        "\n",
        "            x_scale = input_size[0] / image_shape[1]\n",
        "            y_scale = input_size[1] / image_shape[0]\n",
        "            bbox = [bbox[0] * x_scale, bbox[1] * y_scale, bbox[2] * x_scale, bbox[3] * y_scale]\n",
        "\n",
        "            boxes.append(bbox)\n",
        "            labels.append(class_id)\n",
        "\n",
        "    return boxes, labels\n",
        "\n",
        "# Custom Dataset\n",
        "class CTXUXODataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, images_dir, labels_dir, transform=None):\n",
        "        self.images_dir = images_dir\n",
        "        self.labels_dir = labels_dir\n",
        "        self.transform = transform\n",
        "        self.image_files = [f for f in os.listdir(images_dir) if f.endswith('.jpg')]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = os.path.join(self.images_dir, self.image_files[idx])\n",
        "        label_path = os.path.join(self.labels_dir, os.path.splitext(self.image_files[idx])[0] + '.txt')\n",
        "\n",
        "        # Load Image\n",
        "        image = cv2.imread(image_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        original_shape = image.shape[:2]\n",
        "\n",
        "        # Convert YOLO Segments (Polygons) by calling predefined function\n",
        "        boxes, labels = load_yolo_labels_with_polygons(label_path, original_shape)\n",
        "        boxes = torch.tensor(boxes, dtype=torch.float32)\n",
        "        labels = torch.tensor(labels, dtype=torch.int64)\n",
        "\n",
        "        if len(boxes) == 0:  # Dacă nu sunt obiecte, creează un tensor gol\n",
        "            boxes = torch.empty((0, 4), dtype=torch.float32)  # 0 obiecte, 4 coordonate\n",
        "            labels = torch.empty((0,), dtype=torch.int64)      # 0 obiecte\n",
        "\n",
        "\n",
        "        # Transformă the image and labels\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        target = {\n",
        "            'boxes': boxes,\n",
        "            'labels': labels\n",
        "        }\n",
        "           #         'image_id': torch.tensor([idx]),\n",
        "    #        'area': (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1]),\n",
        "     #       'objects_count': len(labels)\n",
        "\n",
        "        return image, target\n",
        "\n",
        "# Copy Paste on new instance (mixup Albumentations)\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize(input_size),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Dataset Config\n",
        "def create_datasets(images_dir, labels_dir, train_split=0.8):\n",
        "    image_files = [f for f in os.listdir(images_dir) if f.endswith('.jpg') or f.endswith('.png')]\n",
        "    num_train = int(len(image_files) * train_split)\n",
        "\n",
        "    train_dataset = CTXUXODataset(images_dir, labels_dir, transform=transform)\n",
        "    val_dataset = CTXUXODataset(images_dir, labels_dir, transform=transform)\n",
        "\n",
        "    train_dataset.image_files = image_files[:num_train]\n",
        "    val_dataset.image_files = image_files[num_train:]\n",
        "\n",
        "    return train_dataset, val_dataset\n",
        "\n",
        "# Specificați calea către imaginile și etichetele dataset-ului\n",
        "def get_dataset() ->list[str] :\n",
        "   images_dir = '/content/UXO-App-Detection-55/train/images'\n",
        "   labels_dir = '/content/UXO-App-Detection-55/train/labels'\n",
        "\n",
        "   return images_dir, labels_dir\n",
        "\n",
        "images_dir, labels_dir=get_dataset()\n",
        "\n",
        "# Crearea dataset-urilor de antrenare și validare\n",
        "train_dataset, val_dataset = create_datasets(images_dir, labels_dir)\n",
        "\n",
        "# Exemplu de testare a dataset-ului\n",
        "def test_dataset(dataset):\n",
        "    for image, target in dataset:\n",
        "        print(f\"Image shape: {image.shape}, Target: {target}\")\n",
        "        break  # Doar primul exemplu\n",
        "\n",
        "# Apel pentru testare\n",
        "test_dataset(train_dataset)  # Pentru a testa dataset-ul de antrenare\n",
        "test_dataset(val_dataset)     # Pentru a testa dataset-ul de validare\n",
        "\n",
        "# Puteți folosi acum dataset-urile în DataLoader pentru antrenare\n",
        "train_data_loader = torch.utils.data.DataLoader(train_dataset,\n",
        "                                                batch_size=16,\n",
        "                                                shuffle=True,\n",
        "                                                collate_fn=collate_fn )\n",
        "val_data_loader = torch.utils.data.DataLoader(val_dataset,\n",
        "                                              batch_size=16,\n",
        "                                              shuffle=False,\n",
        "                                              collate_fn=collate_fn )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHd2hJGRVNnG",
        "outputId": "8b3e9009-d680-42a5-c44e-5eb9c08e3648"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image shape: torch.Size([3, 256, 256]), Target: {'boxes': tensor([[0.0345, 0.0649, 0.0479, 0.0762]]), 'labels': tensor([0])}\n",
            "Image shape: torch.Size([3, 256, 256]), Target: {'boxes': tensor([[0.0656, 0.0607, 0.0951, 0.0687],\n",
            "        [0.0261, 0.0523, 0.0417, 0.0703]]), 'labels': tensor([0, 0])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Image Complexity"
      ],
      "metadata": {
        "id": "H7_GupPduNmI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from meaningful_image_complexity.measure_complexity import ComplexityMeasurer\n",
        "import numpy as np\n",
        "\n",
        "def complexity_gate(image_path):\n",
        "    comp_meas = ComplexityMeasurer(ncs_to_check=8,\n",
        "                                  n_cluster_inits=1,\n",
        "                                  nz=2,\n",
        "                                  num_levels=4,\n",
        "                                  cluster_model='GMM',\n",
        "                                  info_subsample=0.3,\n",
        "                                  )\n",
        "\n",
        "    img = np.load(image_path)\n",
        "\n",
        "    complexity_of_img_at_each_level = comp_meas.interpret(img)\n",
        "    return complexity_of_img_at_each_level"
      ],
      "metadata": {
        "id": "0tySQ7ltuLTR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Moes"
      ],
      "metadata": {
        "id": "fthP6E5IoebO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZqwndAPQTEQf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models, transforms\n",
        "from torchvision.models.detection import ssdlite320_mobilenet_v3_large, fasterrcnn_mobilenet_v3_large_fpn, ssdlite320_mobilenet_v3_large\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "import torch.optim as optim\n",
        "import cv2\n",
        "import numpy as np\n",
        "from vision.references.detection import utils\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from meaningful_image_complexity.measure_complexity import ComplexityMeasurer\n",
        "\n",
        "\n",
        "# Setări generale\n",
        "input_size = (256, 256)\n",
        "num_classes = 2  # UXO and Background\n",
        "\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize(input_size),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Gating Network pentru imagini complexe (region proposal)\n",
        "class GatingNetworkComplex(nn.Module):\n",
        "    def __init__(self, num_experts=10, input_dim=input_size[0]):\n",
        "        super(GatingNetworkComplex, self).__init__()\n",
        "        self.fc = nn.Linear(input_dim, num_experts)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.softmax(self.fc(x), dim=1)\n",
        "\n",
        "# Gating Network pentru imagini simple (SSD)\n",
        "class GatingNetworkSimple(nn.Module):\n",
        "    def __init__(self, num_experts=10, input_dim=input_size[0]):\n",
        "        super(GatingNetworkSimple, self).__init__()\n",
        "        self.fc = nn.Linear(input_dim, num_experts)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.softmax(self.fc(x), dim=1)\n",
        "\n",
        "# Expert bazat pe SSD (MobileNetV3) pentru imagini simple\n",
        "\n",
        "# Expert bazat pe Faster R-CNN (MobileNetV3) pentru imagini complexe\n",
        "class ExpertComplexRegionProposal(nn.Module):\n",
        "    def __init__(self, num_classes=1):\n",
        "        super(ExpertComplexRegionProposal, self).__init__()\n",
        "         # Inițializează modelul SSDLite\n",
        "        self.model = fasterrcnn_mobilenet_v3_large_fpn(weights=\"DEFAULT\")\n",
        "        in_features = self.model.roi_heads.box_predictor.cls_score.in_features\n",
        "        self.model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "    def forward(self, x, targets=None):\n",
        "        # Dacă se oferă ținte, se antrenează modelul\n",
        "        if targets is not None:\n",
        "            loss_dict = self.model(x, targets)\n",
        "            return loss_dict\n",
        "        else:\n",
        "            # Se fac predicții\n",
        "            predictions = self.model(x)\n",
        "            return predictions\n",
        "\n",
        "class ExpertSimpleSSD(nn.Module):\n",
        "    def __init__(self, num_classes=1):\n",
        "        super(ExpertSimpleSSD, self).__init__()\n",
        "         # Inițializează modelul SSDLite\n",
        "        self.model = ssdlite320_mobilenet_v3_large(weights=\"DEFAULT\")\n",
        "        in_features = self.model.head.classification_head.conv[0].in_channels\n",
        "        # Schimbă predicatorul de boxe pentru a se potrivi cu numărul de clase\n",
        "        self.model.head.classification_head = SSDLitePredictor(in_features, num_classes)\n",
        "\n",
        "    def forward(self, x, targets=None):\n",
        "        # Dacă se oferă ținte, se antrenează modelul\n",
        "        if targets is not None:\n",
        "            loss_dict = self.model(x, targets)\n",
        "            return loss_dict\n",
        "        else:\n",
        "            # Se fac predicții\n",
        "            predictions = self.model(x)\n",
        "            return predictions\n",
        "\n",
        "\n",
        "# Modelul complet Adaptive Mixture of Experts\n",
        "class AdaptiveMixtureOfExperts(nn.Module):\n",
        "    def __init__(self, threshold=0.5, num_experts=10, top_k=3):\n",
        "        super(AdaptiveMixtureOfExperts, self).__init__()\n",
        "        self.threshold = threshold\n",
        "        self.gating_complex = GatingNetworkComplex(num_experts=num_experts)\n",
        "        self.gating_simple = GatingNetworkSimple(num_experts=num_experts)\n",
        "        # Experții\n",
        "        self.experts_complex = nn.ModuleList([ExpertComplexRegionProposal() for _ in range(num_experts // 2)])  # 5 experți pentru imagini complexe\n",
        "        self.experts_simple = nn.ModuleList([ExpertSimpleSSD() for _ in range(num_experts // 2)])  # 5 experți pentru imagini simple\n",
        "\n",
        "    def forward(self, x, complexity_score=0.6):\n",
        "        # Alege rețeaua de gating și experții în funcție de complexitate\n",
        "        complexity_score=0.6 #TODO\n",
        "        if complexity_score >= self.threshold:\n",
        "            gating_network = self.gating_complex\n",
        "            experts = self.experts_complex\n",
        "        else:\n",
        "            gating_network = self.gating_simple\n",
        "            experts = self.experts_simple\n",
        "\n",
        "        # Extrage caracteristici pentru gating\n",
        "        output = experts[0].model.backbone(x)\n",
        "        print(type(output))\n",
        "        print(output.keys())\n",
        "        features = output['1'].mean(dim=[2, 3])  # Caracteristici din backbone\n",
        "        gating_weights = gating_network(features)\n",
        "\n",
        "        # Selectează top K experți\n",
        "        top_k_scores, top_k_indices = torch.topk(gating_weights, 3, dim=1)\n",
        "\n",
        "        # Combina rezultatele experților\n",
        "        outputs = torch.stack([expert(x) for expert in experts], dim=1)\n",
        "        weighted_output = (outputs * gating_weights.unsqueeze(2)).sum(dim=1)\n",
        "\n",
        "        return weighted_output\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq, scaler=None):\n",
        "    model.train()\n",
        "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
        "    metric_logger.add_meter(\"lr\", utils.SmoothedValue(window_size=1, fmt=\"{value:.6f}\"))\n",
        "    header = f\"Epoch: [{epoch}]\"\n",
        "\n",
        "    lr_scheduler = None\n",
        "    if epoch == 0:\n",
        "        warmup_factor = 1.0 / 1000\n",
        "        warmup_iters = min(1000, len(data_loader) - 1)\n",
        "\n",
        "        lr_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
        "            optimizer, start_factor=warmup_factor, total_iters=warmup_iters\n",
        "        )\n",
        "\n",
        "    for images, targets in metric_logger.log_every(data_loader, print_freq, header):\n",
        "        images = list(image.to(device) for image in images)\n",
        "        targets = [{k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in t.items()} for t in targets]\n",
        "        with torch.cuda.amp.autocast(enabled=scaler is not None):\n",
        "            loss_dict = model(images, targets)\n",
        "            losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "        # reduce losses over all GPUs for logging purposes\n",
        "        loss_dict_reduced = utils.reduce_dict(loss_dict)\n",
        "        losses_reduced = sum(loss for loss in loss_dict_reduced.values())\n",
        "\n",
        "        loss_value = losses_reduced.item()\n",
        "\n",
        "        if not math.isfinite(loss_value):\n",
        "            print(f\"Loss is {loss_value}, stopping training\")\n",
        "            print(loss_dict_reduced)\n",
        "            sys.exit(1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        if scaler is not None:\n",
        "            scaler.scale(losses).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            losses.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        if lr_scheduler is not None:\n",
        "            lr_scheduler.step()\n",
        "\n",
        "        metric_logger.update(loss=losses_reduced, **loss_dict_reduced)\n",
        "        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n",
        "\n",
        "    return metric_logger\n",
        "\n",
        "# Exemplu de apel al funcției\n",
        "# train_model(train_loader)  # Specificați train_loader\n",
        "# test_single_image(\"cale/catre/imagine.jpg\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jAtOvXlCsbSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "def train_complex_expert():\n",
        "    print(\"Start training for complex expert\")\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    # model = AdaptiveMixtureOfExperts(threshold=0.5, num_experts=10).to(device)\n",
        "    model=ExpertComplexRegionProposal().to(device)\n",
        "    epochs=5\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    params = [p for p in model.parameters() if p.requires_grad]\n",
        "    optimizer = optim.SGD(params,lr=0.0005,momentum=0.9,weight_decay=0.0005)\n",
        "    lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
        "        optimizer,\n",
        "        step_size=50,\n",
        "        gamma=0.1\n",
        "    )\n",
        "    print_freq=20\n",
        "    output_dir=\"./results\"\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "    start_time = time.time()\n",
        "    for epoch in range(epochs):\n",
        "        train_one_epoch(model, optimizer, train_data_loader, device, epoch, print_freq, scaler)\n",
        "        lr_scheduler.step()\n",
        "        if output_dir:\n",
        "            checkpoint = {\n",
        "                \"model\": model_without_ddp.state_dict(),\n",
        "                \"optimizer\": optimizer.state_dict(),\n",
        "                \"lr_scheduler\": lr_scheduler.state_dict(),\n",
        "                \"epoch\": epoch,\n",
        "                \"epochs\":epochs\n",
        "            }\n",
        "            utils.save_on_master(checkpoint, os.path.join(output_dir, f\"model_{epoch}.pth\"))\n",
        "            utils.save_on_master(checkpoint, os.path.join(output_dir, \"checkpoint.pth\"))\n",
        "\n",
        "        # evaluate after every epoch\n",
        "        #evaluate(model, data_loader_test, device=device)\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "    total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
        "    print(f\"Training time {total_time_str}\")\n"
      ],
      "metadata": {
        "id": "8012WVIhrq1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_complex_expert()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAsY2otxSg2o",
        "outputId": "c90f401c-c7cb-4cd6-97df-cd1856b34d94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start training for complex expert\n",
            "Epoch: [0]  [  0/157]  eta: 1:50:57  lr: 0.000004  loss: 0.4570 (0.4570)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.0000 (0.0000)  loss_objectness: 0.2237 (0.2237)  loss_rpn_box_reg: 0.2333 (0.2333)  time: 42.4063  data: 2.0506\n",
            "Epoch: [0]  [ 20/157]  eta: 1:35:33  lr: 0.000068  loss: 0.4567 (0.4562)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.0000 (0.0000)  loss_objectness: 0.2225 (0.2223)  loss_rpn_box_reg: 0.2329 (0.2339)  time: 41.8195  data: 1.4360\n"
          ]
        }
      ]
    }
  ]
}